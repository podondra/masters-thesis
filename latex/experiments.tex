\chapter{Experiments with Deep Domain Adaptation}
\label{exp_chapter}

In previous chapters, we have selected suitable astronomical data, surveyed and chosen suitable deep domain adaptation methods. Now, we carry out experiments with the DDC, Deep CORAL, DANN and DRCN on astronomical spectra from the SDSS and LAMOST spectroscopic sky surveys.

Firstly, we create the source and target datasets for experiments. Then, we use PCA, t-SNE and UMAP to reduce the data to two-dimensions, so we can visualise the data and investigate distributions of both the source and~target data. Thirdly, we introduce a CNN baseline model which serves as a~benchmark for comparison of the performance of the deep domain adaptation methods. Finally, we employ four deep domain adaptation methods and~evaluate the adapted results to see if astronomical spectroscopy can benefit from deep domain adaptation.

\section{Data Preparation}
\label{data_preparation}

Our data of source domain consists of 4\,851\,200 optical spectra from the SDSS DR14 catalogue
and the corresponding SDSS DR14Q catalogue of 649\,791 spectra of 526\,356 QSO objects
(we have to distinguish an object and a spectrum
because each astronomical object could be observed multiple times having multiple spectra).
Both catalogues are introduced in~Subsection~\ref{sdss}.
However, 20\,279 spectra of QSOs cannot be identified
% TODO describe the bug in SDSS DR14Q catalogue
because there is a bug in the SDSS DR14Q catalogue.
Therefore, we can identify only 629\,512 spectra of all QSOs.
Next, we need to cross-match the SDSS DR14 and DR14Q catalogues
to merge the data stored in individual FITS files with QSO labels.
The cross-matching is based on a triplet of a \textit{plate number}, a \textit{Modified Julian Date of observation} and a \textit{fibre number}
that is unique to each spectrum.
Additionally to the 20\,279 spectra of QSOs lost due to the bug in the DR14Q catalogue,
we were unable to cross-match 55 QSOs with the SDSS DR14 catalogue.
Therefore, we have 629\,457 spectra of QSOs
for which we have actual data in FITS files and not only metadata in catalogues.

The complement to the source domain in domain adaptation is the target domain.
We selected data from LAMOST DR5 to be target domain data
for~reasons described in Subsection~\ref{lamost}.
The LAMOST DR5 general catalogue contains 9\,026\,365 spectra,
and the complete catalogue of QSOs has 42\,552 spectra.
Again, we cross-matched the LAMOST DR5 catalogue and~the~catalogue of QSOs
according to a quartet of a \textit{plan identifier}, a \textit{local Modified Julian Date} (one less the Modified Julian Date), a \textit{spectrograph identifier} and an \textit{identifier of fibre}.
We were able to cross-match 31\,755 spectra of QSOs with the general catalogue
effectively losing 10\,797 spectra of QSOs.
We believe that LAMOST has sound reasons for not including those spectra in~the~LAMOST DR5 catalogue.

However, the labels of QSOs from LAMOST are incompatible with labels from SDSS
because the criteria of what is a QSO are different in SDSS and~LAMOST
(see Section~\ref{large_spec_surveys}).
For us, the ground truths are labels of SDSS DR14Q catalogue
while the labels of LAMOST serves only for evaluation purposes,
not for training.
Therefore, there might be spectra truly QSOs in~LAMOST
not yet identified by LAMOST biasing our performance metrics.

Having assigned labels of QSOs to individual spectra,
we need to extract the spectra from individual FITS files
because learning of neural networks requires datasets to be in the form of design matrices.
A design matrix contains a different example (a spectrum) in each row.
In contrast, each column of~the~design matrix corresponds to a different feature
(a measurement of flux in~a~specific wavelength).~\cite{goodfellow2016}

Fortunately, SDSS and LAMOST spectra have a common wavelength grid in logarithmic wavelengths evenly space by 0.0001.
Although all spectra have a common wavelength grid,
the minimal and maximal wavelengths are different for each spectrum.
Figure~\ref{wavemin_wavemax_hist} displays histograms of minimal and maximal wavelengths of all LAMOST spectra.
In this work, we aim to find QSOs in~the~LAMOST DR5.
Therefore, we would like to keep as many spectra as possible from the LAMOST~DR5.
To keep all spectra from the LAMOST~DR5,
we have to select wavelength range starting at 3\,839.7244~\AA{} (3.5843 in logarithmic wavelength)
which is the maximum from minimal wavelengths
and ending at 8\,914.597~\AA{} (3.9501 in logarithmic wavelength)
which is the minimum from maximal wavelengths.
The selection gives us a wavelength grid of 3659 logarithmically-spaced wavelengths
(each spectrum is a real vector of~\(\mathbb{R}^{3659}\)).

\begin{figure}
\includegraphics[width=\textwidth]{img/wavemin_wavemax_hist.pdf}
\caption[Minimal and maximal wavelength of LAMOST DR5]{
	Two plots showing histograms of minimal and maximal wavelengths
	of all LAMOST spectra.
	The maximal wavelength from all the minimal wavelengths
	is 3\,839.7244~\AA{}.
	Cutting in a lower wavelength
	would mean a loss of almost 100\,000 spectra.
	The situation is very similar for maximal wavelengths,
	where the minimum is 8\,914.597~\AA{}.
	Therefore, the most suitable range of wavelength is to choose
	these two wavelengths as starting and ending points.
	}
\label{wavemin_wavemax_hist}
\end{figure}

Given the selected grid of wavelengths,
we will lose some SDSS spectra because not all of them have all measurements in the range.
Figure~\ref{waves_cumulative_hist} shows the cumulative histogram of how many spectra we will keep for cuts in different wavelengths.
We see that significant drops are behind the selected minimal and maximal wavelengths
that mean we will keep most of the spectra.
Precisely, the cut will drop 34\,487 spectra from our source dataset
including 1\,949 spectra of QSO.
Therefore, the source dataset has 4\,816\,713 spectra with~627\,508 spectra of QSOs that can enter learning of a neural network.

\begin{figure}
\includegraphics[width=\textwidth]{img/waves_cumulative_hist.pdf}
\caption[Losts in SDSS DR14 spectra due to wavelength range]{
	The selected wavelength range will inevitably
	cause a loss of some SDSS DR14 spectra.
	This figure shows cumulative histograms of the number of spectra
	and its dependence on minimal and maximal wavelengths.
	We see that both cuts are before the big drop is the count of spectra.
	}
\label{waves_cumulative_hist}
\end{figure}

The original sizes of data are unnecessary for experimenting with deep domain adaptation on astronomical spectra.
We store each spectrum as a vector of 3\,659 single-precision floating-point number (4 bytes).
The storage setting gives that the SDSS source dataset has about 70.5~GB
and the LAMOST target dataset 132.1~GB.
Data of such size usually cannot fit into memory,
and access to a disk significantly slows learning on a GPU.

Therefore, we have subsampled the data to the size of ImageNet~\cite{russakovsky2015}.
We believe that the size of ImageNet is reasonable
because ImageNet is the~dataset that enables the superiority of deep neural network in computer vision.
ImageNet has 1 million training examples, 50 thousand validation examples
and~100 thousand testing examples.
Accordingly, we randomly subsampled of~source and target datasets
obtaining training sets of size 1 million
and validation sets of size 50 thousand.
At the same time, the rest of the data would serve as testing sets.
We summarise sizes of datasets with the corresponding number of QSOs in Table~\ref{datasets_sizes}.
Table~\ref{datasets_sizes} shows a significant class imbalance in the LAMOST DR5,
where QSOs are very rare (less than 0.4\%).

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|}
	\hline
	Name & Number of QSO spectra & Total spectra \\ \hline \hline
	SDSS DR14 & 629\,457 (12.98\%) & 4\,851\,200 \\ \hline
	usable SDSS DR14 & 627\,508 (13.03\%) & 4\,816\,713 \\ \hline
	SDSS training set & 130\,904 (13.09\%) & 1\,000\,000 \\ \hline
	SDSS validation set & 6\,552 (13.10\%) & 50\,000 \\ \hline
	LAMOST DR5 v3 & 31\,755 (0.35\%) & 9\,026\,365 \\ \hline
	LAMOST training set & 3\,517 (0.35\%) & 1\,000\,000 \\ \hline
	LAMOST validation set & 190 (0.38\%) & 50\,000 \\ \hline
\end{tabular}
\end{center}
\caption[Sizes of source and target datasets]{
	Summary table of sizes of the source and target dataset
	together with train and validation splits.
	The validation splits serve for models comparison
	and hyperparameter optimisation.
	The second row shows the number of spectra
	after the cut into a unified range of wavelengths.
	The table shows the imbalance of the LAMOST target data
	that contains only a tiny amount of identified QSOs.
	}
\label{datasets_sizes}
\end{table}

The last step of data preparation is min-max scaling of each spectrum into~the~\([-1; 1]\) range:

\begin{equation}
	\mathbf{x}_i = 2 \frac{\mathbf{x}_i - \min(\mathbf{x}_i)}{
		\max(\mathbf{x}_i) - \min(\mathbf{x}_i)} - 1,
\end{equation}

where \(\mathbf{x}_i \in \mathbb{R}^{3659}\) is a spectrum as defined in Chapter~\ref{da_chapter}
and functions \(\min(\cdot)\) and \(\max(\cdot)\) returns the smallest and the largest element of a given vector, respectively.

There are two benefits of the min-max scaling.
Firstly, the data will be in a suitable range for the learning of neural networks
which will stabilise learning.
Secondly, the scaling will remove intensity properties of spectra,
leaving us only with the spectrum shape
which we want to use for identification of QSOs.

\section{Dimensionality Reduction}

In this section, we investigate the structure of joint data space of source and~target datasets with three dimensionality reduction methods:
\textit{principal component analysis},
\textit{t-Distributed Stochastic Neighbor Embedding}
and \textit{Uniform Manifold Approximation and Projection} (UMAP).
We would like to get an idea of how well are the source and target data mixed,
if there are some separate clusters or the data are rather continuos.

To avoid visualisation overwhelmed with data points,
we sampled 2\,500 spectra from the source training set
and 2\,500 spectra from the target training set.
The spectra are min-max scaled, not standardised
because the relation between features is meaningful,
and we do not want to suppress the relationship.

\textit{Principal component analysis} (PCA) is a simple linear machine learning algorithm
that is used either for visualisation or feature extraction by dimensionality reduction.
PCA learns a representation whose features are uncorrelated with each other
and selects features with the largest variation.~\cite{goodfellow2016}
We show the visualisation obtained with~PCA in Figure~\ref{pca}.
The plot shows that source data tent to concentrate in the middle
while the target data are on the edges.
However, no regions are containing only the source or target data.
Moreover, in the middle extending to the right, there is a kind of line component.

\begin{figure}
\includegraphics[width=\textwidth]{img/pca.pdf}
\caption[PCA visualisation of source and target data distributions]{
	The first two principal components of 2\,500 source
	and 2\,500 target data points.
	The projection shows that source data concentrate more in~the~middle
	while the target data seem to cluster on the edges.
	}
\label{pca}
\end{figure}

\textit{t-Distributed Stochastic Neighbor Embedding} (t-SNE)~\cite{maaten2008, wattenberg2016} is a popular method for visualisation of high-dimensional data.
The t-SNE method is non-linear, iterative
and performs different transformations of different regions.
A tunable hyperparameter of t-SNE is \textit{perplexity}
which is a guess about~the~number of neighbours of a data point.
Typically, the optimal value is between 5 and 50.

We reduce dimensionality for perplexities from \(\{5, 10, 30, 50, 100\}\).
The~best result was for the value 50,
and the result is shown in Figure~\ref{tsne}.
The t-SNE embedding has a similar structure as Figure~\ref{pca} of PCA.
There are mostly source data in the centre and target data around it.
However, the~separation between the centre and edges seems to be larger.
Still, there is the line component extending downward this time.

t-SNE is often used in the papers presenting a deep domain adaptation methods
to show how feature extracted from a higher layer in an adapted network
are better mix when a domain adaptation method is employed.
But, when a network is not adapted the source and target data
can be easily visually separated in a t-SNE visualisation.

\begin{figure}
\includegraphics[width=\textwidth]{img/tsne.pdf}
\caption[t-SNE visualisation of source and target data distributions]{
	Embedding of t-SNE of the same data
	as in the reduction with~PCA shows the very similar result as PCA.
	However, there is more notable central sort of line component
	extending downward.
	}
\label{tsne}
\end{figure}

\textit{Uniform Manifold Approximation and Projection} (UMAP)~\cite{mcinnes2018}
is a non-linear dimensionality reduction algorithm based on manifold learning and ideas from topological data analysis.
It achieves visualisations similar to t-SNE, but it is significantly faster.
Visualisation with UMAP is displayed in~Figure~\ref{umap}
and has a very similar structure to t-SNE embedding in Figure~\ref{tsne}.

\begin{figure}
\includegraphics[width=\textwidth]{img/umap.pdf}
\caption[UMAP visualisation of source and target data distributions]{
	UMAP projection to two-dimensions confirms the previous visualisation
	with~PCA and t-SNE.
	The source data tend to concentrate in~the~middle
	while the target data are mostly out of the centre,
	and there is the line component extending away from the centre.
	}
\label{umap}
\end{figure}

\section{Baseline: Results without Deep Domain Adaptation}
\label{baseline}

Now, we are ready for training of neural networks.
However, before we dive into deep domain adaptation,
we will train a classical convolutional network
which will serve as a baseline to which we can compare results of networks augmented for deep domain adaptation.

As the baseline, we choose LeNet-5~\cite{lecun1998} convolutional neural network,
which was initially used to recognise handwritten digits of MNIST~\cite{lecun1998}.
We have chosen the architecture of LeNet-5
because it is the simplest model used in the DANN paper~\cite{ganin2016}.
Thus, we will not need to create our architecture for the DANN experiment.
However, the network is designed for processing of two-dimensional images
while a spectrum is a one-dimensional image.
Therefore, we have to substitute the two-dimensional convolutions with one-dimensional convolutions.
Moreover, we increased the kernel size and stride of pooling layers from 2 to 16
so that the output of the convolutional layers is reasonably big.
If we left the original pooling layers,
the input to the first fully connected layer would be of size 43\,872
in comparison to the original input size 768 for the MNIST dataset.
The kernel size and stride of 16 will reduce the input size of our network to 672.
Figure~\ref{lenet_5} displays the final architecture,
which we implemented in PyTorch~\cite{paszke2019} like all other models.

\tikzstyle{layer} = [align=center, draw=black, font=\tiny, rectangle]
\begin{figure}
\begin{center}
\subfloat[
	The architecture of our LeNet-5 contains two one-dimensional convolutional layers and three fully-connected layers.
	Both convolutional layers have a kernel size of 5 and are followed by a pooling layer.
	The first one has 32  and the second 48 channels.
	The two first fully-connected layers have outputs of sizes 100.
	All activation are ReLU except for the last one,
	which is sigmoid because our classification problem is binary.
]{
\begin{tikzpicture}[node distance=1.5cm]
	\node (conv1) [layer] {conv1\\5-32\\ReLU};
	\node (pool1) [layer,right of=conv1] {pool1\\16-16};
	\node (conv2) [layer,right of=pool1] {conv2\\5-48\\ReLU};
	\node (pool2) [layer,right of=conv2] {pool2\\16-16};
	\node (fc1) [layer,right of=pool2] {fc1\\100\\ReLU};
	\node (fc2) [layer,right of=fc1] {fc2\\100\\ReLU};
	\node (fc3) [layer,right of=fc2] {fc3\\1\\sigmoid};
	\draw [->] (conv1) -- (pool1);
	\draw [->] (pool1) -- (conv2);
	\draw [->] (conv2) -- (pool2);
	\draw [->] (pool2) -- (fc1);
	\draw [->] (fc1) -- (fc2);
	\draw [->] (fc2) -- (fc3);
\end{tikzpicture}
\label{lenet_5}
}\\
\subfloat[
	The architecture DDC is almost the same as our LeNet-5.
	It has only a fully-connected adaptation layer (bottleneck) of output size 64 inserted after the first fully-connected layer.
]{
\begin{tikzpicture}[node distance=1.5cm]
	\node (conv1) [layer] {conv1\\5-32\\ReLU};
	\node (pool1) [layer,right of=conv1] {pool1\\16-16};
	\node (conv2) [layer,right of=pool1] {conv2\\5-48\\ReLU};
	\node (pool2) [layer,right of=conv2] {pool2\\16-16};
	\node (fc1) [layer,right of=pool2] {fc1\\100\\ReLU};
	\node (bottleneck) [layer,right of=fc1,fill=red!50] {bottleneck\\64\\ReLU};
	\node (fc2) [layer,right of=bottleneck] {fc2\\100\\ReLU};
	\node (fc3) [layer,right of=fc2] {fc3\\1\\sigmoid};
	\draw [->] (conv1) -- (pool1);
	\draw [->] (pool1) -- (conv2);
	\draw [->] (conv2) -- (pool2);
	\draw [->] (pool2) -- (fc1);
	\draw [->] (fc1) -- (bottleneck);
	\draw [->] (bottleneck) -- (fc2);
	\draw [->] (fc2) -- (fc3);
\end{tikzpicture}
\label{ddc_architecture}
}\\
\subfloat[
	The architecture of DANN is composed of a feature extractor (white), label predictor (blue) and domain classifier (green) with~the~GRL layer.
]{
\begin{tikzpicture}[node distance=1.5cm]
	\node (conv1) [layer] {conv1\\5-32\\ReLU};
	\node (pool1) [layer,right of=conv1] {pool1\\16-16};
	\node (conv2) [layer,right of=pool1] {conv2\\5-48\\ReLU};
	\node (pool2) [layer,right of=conv2] {pool2\\16-16};
	\node (fc1) [layer,right of=pool2,fill=blue!50] {fc1\\100\\ReLU};
	\node (fc2) [layer,right of=fc1,fill=blue!50] {fc2\\100\\ReLU};
	\node (fc3) [layer,right of=fc2,fill=blue!50] {fc3\\1\\sigmoid};
	\node (grl) [layer,below of=pool2,fill=green!50] {gradient\\reversar\\layer};
	\node (fc4) [layer,right of=grl,fill=green!50] {fc4\\100\\ReLU};
	\node (fc5) [layer,right of=fc4,fill=green!50] {fc5\\1\\sigmoid};
	\draw [->] (conv1) -- (pool1);
	\draw [->] (pool1) -- (conv2);
	\draw [->] (conv2) -- (pool2);
	\draw [->] (pool2) -- (fc1);
	\draw [->] (fc1) -- (fc2);
	\draw [->] (fc2) -- (fc3);
	\draw [->] (pool2) -- (grl);
	\draw [->] (grl) -- (fc4);
	\draw [->] (fc4) -- (fc5);
\end{tikzpicture}
\label{dann_architecture}
}\\
\subfloat[
	The architecture of DRCN contains an encoder (white), decoder (green) with an upsampling layer and classifier (blue).
]{
\begin{tikzpicture}[node distance=1.5cm]
	% encoder
	\node (conv1) [layer] {conv1\\5-32\\ReLU};
	\node (pool1) [layer,right of=conv1] {pool1\\16-16};
	\node (conv2) [layer,right of=pool1] {conv2\\5-48\\ReLU};
	% feature labelling
	\node (pool2) [layer,right of=conv2,fill=blue!50] {pool2\\16-16};
	\node (fc1) [layer,right of=pool2,fill=blue!50] {fc1\\100\\ReLU};
	\node (fc2) [layer,right of=fc1,fill=blue!50] {fc2\\100\\ReLU};
	\node (fc3) [layer,right of=fc2,fill=blue!50] {fc3\\1\\sigmoid};
	% decoder
	\node (conv3) [layer,below of=conv2,fill=green!50] {conv3\\5-48\\ReLU};
	\node (upsample1) [layer,left of=conv3,fill=green!50] {upsample1\\3659};
	\node (conv4) [layer,left of=upsample1,fill=green!50] {conv4\\5-32\\None};
	% connections
	\draw [->] (conv1) -- (pool1);
	\draw [->] (pool1) -- (conv2);
	\draw [->] (conv2) -- (pool2);
	\draw [->] (pool2) -- (fc1);
	\draw [->] (fc1) -- (fc2);
	\draw [->] (fc2) -- (fc3);
	\draw [->] (conv2) -- (conv3);
	\draw [->] (conv3) -- (upsample1);
	\draw [->] (upsample1) -- (conv4);
\end{tikzpicture}
\label{drcn_architecture}
}
\end{center}
\caption[Experimental architectures of deep domain adaptation models]{
	Diagrams of all architectures used in our experiments.
	Note that the architectures are designed in such way
	that the classificator is the same for all models
	(minor exception is the adaptation layer in DDC).
	}
\end{figure}

Donahue et al. showed in the \textit{Deep Convolutional Activation Feature} (DeCAF) paper~\cite{donahue2014}
that features extracted from a deep CNN can be repurposed to novel tasks
if the network was trained on a large fixed set in a fully supervised fashion,
which means that it can be used for domain adaptation on its own.
Therefore, we can expect that our CNN trained on the SDSS
will be able to find features beneficial for domain adaptation.
Still, the DeCAF paper shows that a deep CNN cannot remove domain bias completely.
Therefore, there is a space for improvement with deep domain adaptation.

\begin{figure}
\begin{center}
\subfloat[
	The training and validation losses are similar proving no overfitting.
]{\includegraphics[width=.75\textwidth]{img/lenet_losses.pdf}}\\
\subfloat[
	The source \(F_1\) score is gradually improving
	while the target \(F_1\) score suffers.
]{\includegraphics[width=.75\textwidth]{img/lenet_f1.pdf}}
\end{center}
\caption[Training progress of LeNet-5]{
	Our LeNet-5 is properly learning on the source data.
	However, it is unable to transfer knowledge to the target domain.
}
\label{lenet_losses}
\end{figure}

We trained our augmented LeNet-5 on batches of size 64 for 20 epochs with the Adam optimiser~\cite{kingma2014} in its default setting
and used the \textit{binary cross entropy loss} defined as:

\begin{equation}
	\mathit{BCE}(\theta) = -\frac{1}{M} \sum_i^M [y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i)],
\end{equation}

where \(\theta\) are parameters of a model,
\(M\) is the batch size,
\(y_i \in \{0, 1\}\) is the~true label,
and \(\hat{y}_i \in [0, 1]\) are the model predictions of the \(i\)th example in~the~batch.
Furthermore, we initialised the weights and biases following Xavier initialisation~\cite{glorot2010}.
That is weights of our neural network are sampled from uniform distribution \(\mathcal{U}\):

\begin{equation}
	\mathcal{U}\left(
	-\frac{\sqrt{6}}{\sqrt{\mathit{in} + \mathit{out}}},
	\frac{\sqrt{6}}{\sqrt{\mathit{in} + \mathit{out}}}
	\right),
\end{equation}

where \(\mathit{in}\) is the number of~input units of a layer
and \(\mathit{out}\) is the number of~output units
and biases are set to zero.

Before the analysis of results of LeNet-5,
we define performance metrics commonly used for imbalanced datasets:
\textit{recall} \(r\), \textit{precision} \(p\) and \textit{\(F_1\) score} is the harmonic mean between \textit{precision} and \textit{recall}:

\begin{align}
	r &= \frac{\mathit{TP}}{(\mathit{TP} + \mathit{FN})}, \\
	p &= \frac{\mathit{TP}}{(\mathit{TP} + \mathit{FP})}, \\
	F_1 &= \frac{2}{r^{-1} + p^{-1}} = 2 \frac{r p}{r + p},
\end{align}

where \(r\) is recall, \(p\) is precision,
\(\mathit{TP}\) is the number of correctly classified QSOs,
\(\mathit{FN}\) is the number of QSOs incorrectly classified as non-QSOs,
and \(\mathit{FP}\) is the number of non-QSOs classified as QSOs.
When precision and recall are perfect,
\(F_1\) score reaches its best value one, and at worst can be zero.

Figure~\ref{lenet_losses} displays the training progress of our LeNet-5.
We see that the network has converged and is not overfitting
because the gap between the training and validation loss is small.
The source \(F_1\) score in gradually improving meanwhile, the target \(F_1\) score suffers.

The results\footnote{Note that all metrics (\(F_1\) score, precision, recall and confusion matrices) were computed on either the source or target validation sets in our experiments.} of our baseline are quite good on the source domain
because the source \(F_1\) score is 0.9397
(the source recall is 95.82\% and the source precision is 92.19\%).
However, the target \(F_1\) score is 0.2294
(the target precision is 13.48\% and the target recall 76.84\%).
That is an inferior result for the target data in the light of source performance.
We see that there probably is a considerable domain discrepancy,
and, therefore, an opportunity for domain adaptation.

\begin{table}
\begin{center}
\subfloat[Confusion matrix for the source domain.]{
\begin{tabular}{|l|r|r|}
	\hline
	Predicted class & \multicolumn{2}{c|}{Actual class} \\
	\hline \hline
	& QSO & non-QSO \\ \hline
	QSO & 6\,278 & 532 \\ \hline
	non-QSO & 274 & 42\,916 \\ \hline
\end{tabular}
}
\subfloat[Confusion matrix for the target domain.]{
\begin{tabular}{|l|r|r|}
	\hline
	Predicted class & \multicolumn{2}{c|}{Actual class} \\
	\hline \hline
	& QSO & non-QSO \\ \hline
	QSO & 146 & 937 \\ \hline
	non-QSO & 44 & 48\,873 \\ \hline
\end{tabular}
}
\end{center}
\caption[Confusion matrices of the baseline model]{
	Confusion matrices of the baseline model for the source and target validation sets.
	We see the enormous error on the target domain
	where the model predicts 937 non-QSOs as QSOs and cannot identify 44 QSOs.
}
\end{table}

\section{Experiments with Deep Domain Adaptation}

We set the baseline result with classical CNN.
Now, we apply four deep domain adaptation methods to the same data
to analyse if astronomical spectroscopy can benefit from domain adaptation
based on neural networks.

We start with two discrepancy-based approaches, which are DDC and Deep CORAL.
Then, we continue with DANN, which is an adversarial-based domain adaptation method
and with reconstruction-based DRCN.
We conclude with~the~evaluation and comparison of results.

\subsection{DDC: Deep Domain Confusion}

First deep domain adaptation method is \textit{Deep Domain Confusion} (DDC).
DDC reduces the domain discrepancy (maximises domain confusion)
by extending classification loss of a neural network with the MMD loss
(see Equation~\ref{ddc_loss}).
The MMD loss is enforced on the \textit{adaptation layer}
that serves as an information bottleneck for domain confusion.
More details on DDC are in~Subsection~\ref{discrepancy_da}.

To select the size and placement of the adaptation layer,
we followed the~same procedure as in the DDC paper~\cite{tzeng2014}.
Firstly, we took the LeNet-5 trained in previous Section~\ref{baseline}
and extracted features from the first and second fully-connected layer
(the last fully-connected layer has a trivial width)
for all validation examples.
Then, we computed MMD between the source and~target data at each layer with the extracted features.
The intuition is to place the~adaptation layer after a layer with the smallest MMD
because low MMD means more domain invariant features.
We measured that the MMD at the first fully-connected layer is 50.70,
while MMD at the second fully-connected layer is 53.33.
Therefore, we will place the adaptation layer after the first fully-connected layer.
Secondly, we have to optimise the width of the adaptation layer.
Therefore, we trained the LeNet-5 with the adaptation layer of sizes
from \(\{4, 8, 16, 32, 64\}\), excluding the low width of 2
and not exceeding the output size of the previous layer.
The stepping is the power of two as in~the~original paper.
Figure~\ref{adaptation_layer} plots the resulting MMD values for different setting
and shows that the width 64 is the best.

The final architecture of our DDC network is in Figure~\ref{ddc_architecture}.
It is the baseline CNN with the adaptation layer of width 64
after the first fully-connected layer.

\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth]{img/adaptation_layer_width.pdf}
\end{center}
\caption[Optimisation of the width of the adaptation layer]{
	We employed the same methodology to optimise hyperparametes as the original paper of DDC.
	Therefore, we compute the MMD for different sizes of adaptation layers.
	The scatter plot show that the size of 64 is the best closely followed by the size of 8.
}
\label{adaptation_layer}
\end{figure}

Furthermore, we set the trade-off parameter \(\lambda\)
between the binary cross entropy loss
and the MMD loss to 0.25 as in the DDC paper
and trained the network in the same way as described in the previous Section~\ref{baseline}
(Xavier initialisation, Adam optimiser, batch size 64 and 20 epochs).

\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth]{img/ddc_mmds.pdf}
\end{center}
\caption[Maximum mean discrepancy with and without MMD loss]{
	This plot illustrates that enforcing the MMD loss has the expected effect. On the other hand, without the MMD loss, the MMD between the~source and target domain gradually grows.
}
\label{ddc_mmds}
\end{figure}

The training of DDC proceeded similarly to the baseline.
Moreover, we see that MMD is also minimalised in the bottom plot of Figure~\ref{ddc_mmds}.
On~the~other side, if we train the network without enforcing MMD loss (\(\lambda = 0\)),
then the~MMD is growing, as shown in the top plot of Figure~\ref{ddc_mmds}.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|}
	\hline
	Predicted class & \multicolumn{2}{c|}{Actual class} \\
	\hline \hline
	& QSO & non-QSO \\ \hline
	QSO & 148 & 1\,138 \\ \hline
	non-QSO & 42 & 48\,672 \\ \hline
\end{tabular}
\end{center}
\caption{Confusion matrix of DDC for target data}
\label{ddc_confusion}
\end{table}

Although, training run as expected,
DDC achieved an unfortunate result in~comparison to baseline.
The \(F_1\) score on the source data is 0.9354 and 0.2005 on the target data that is lower than the baseline in both cases.
Precision on~target is 11.51\%,
and the only improvement is the recall of value 77.89\%,
but the decrease in precision is probably caused by that.

\subsection{Deep CORAL: Deep Correlation Alignment}

\textit{Deep Correlation Alignment} (Deep CORAL) is very similar to DDC.
DDC aligns means of the source and target distributions with MMD loss
while Deep CORAL aligns correlations with CORAL loss
(see Equations~\ref{coral_loss} and~\ref{deep_coral_loss}).
Moreover, Deep CORAL applies the CORAL loss straight to a layer in~a~network
not creating an adaptation layer.
We implemented Deep CORAL with inspiration from the original
code\footnote{Available from: \url{https://github.com/visionlearninggroup/CORAL}}
and followed all the steps described in the corresponding paper~\cite{sun2016}.

Originally, the architecture underlying Deep CORAL is AlexNet~\cite{krizhevsky2012}.
There the CORAL loss was put on the last layer that has ten output units.
However, our neural network has to have one output unit,
so applying CORAL loss to it does not make sense.
Therefore, we applied the CORAL loss to the second fully-connected layer
of our LeNet-5 architecture in Figure~\ref{lenet_5}.
Then, we optimised the trade-off between classification and CORAL loss
\(\lambda\) from \(\{0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001\}\).
The best is \(\lambda = 0.0005\),
which makes the classification loss and the CORAL loss of similar magnitude as suggested by the paper
(see Figure~\ref{deep_coral_losses}).
Furthermore, we used the same batch size of value 128
as in original experiments of Deep CORAL.
The architecture is initialised in the same way as our baseline
and optimised with~Adam for 20 epochs.

\begin{figure}
\begin{center}
\subfloat[
	The original paper states that the classification loss and~the~CORAL loss should be almost the~same at~the~end of~the~training.
	We show in this plot that the~similarity can be achieved by~setting \(\lambda = 0.0005\).
	]{
\includegraphics[width=0.75\textwidth]{img/deep-coral_losses.pdf}
\label{deep_coral_losses}
}\\
\subfloat[
	The CORAL statistic without enforcing the CORAL loss grows significantly in comparison to the scenario in the plot above
	when the~network is trained with the CORAL loss.
]{
\includegraphics[width=0.75\textwidth]{img/deep-coral_coral.pdf}
\label{deep_coral_coral}
}
\end{center}
\caption{Training of Deep Correlation Alignment}
\label{deep_coral_training}
\end{figure}

Firstly, we trained our Deep CORAL with \(\lambda = 0\) to see
how the CORAL grows.
Figure~\ref{deep_coral_coral} shows the same behaviour of CORAL
without enforcing the minimisation of correlation loss as in the original paper.
Then, we experimented with the best \(\lambda = 0.0005\)
and obtained the training progress in~Figure~\ref{deep_coral_losses}.
However, the results are unsatisfactory, as in~the~case of~DDC.

Source data \(F_1\) score is 0.9396 that is almost the same as baseline.
There is a small but insignificant improvement in the target \(F_1\) score,
which is of~value 0.2509
(the target precision is 14.97\% and the target recall is 77.37\%).
These values are gains, but they are too small,
and the model with such small precision is useless for identification of QSOs.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|}
	\hline
	Predicted class & \multicolumn{2}{c|}{Actual class} \\
	\hline \hline
	& QSO & non-QSO \\ \hline
	QSO & 147 & 835 \\ \hline
	non-QSO & 43 & 48\,975 \\ \hline
\end{tabular}
\end{center}
\caption{Confusion matrix of Deep CORAL for target data}
\end{table}

\subsection{DANN: Domain-Adversarial Neural Network}

\textit{Domain-Adversarial Neural Network} (DANN) is an adversarial-based domain adaptation method.
Our DANN architecture is depicted in Figure~\ref{dann_architecture}.
It consists of a feature extractor, a predictor and a domain classifier
that acts adversarially against the feature extractor
enforcing domain invariant representation.
Further details are in Subsection~\ref{adversarial_da}.

We code and schedule hyperparameters of DANN according to the original
implementation\footnote{Available from: \url{http://sites.skoltech.ru/compvision/projects/grl/}}
and the two papers where DANN was published~\cite{ganin2016, ganin2015}.
Therefore, we implemented the learning rate schedule for SGD with momentum:

\begin{equation}
	\mu_p = \frac{\mu_0}{(1 + \alpha p)^\beta},
\end{equation}

where \(p\) is the training progress linearly changing from 0 to 1 in every iteration,
initial learning rate \(\mu_0 = 0.01\), \(\alpha = 10\) and \(\beta = 0.75\).
Furthermore, we also implemented the domain adaptation parameter \(\lambda\) from Equation~\ref{dann_loss} that starts at 0 and grows to 1 with the schedule:

\begin{equation}
	\lambda_p = \frac{2}{1 + e^{-\gamma p}} - 1,
\end{equation}

where \(p\) is again the training progress
and \(\gamma\) that was set to 10 as in~the~original paper.

Note that binary cross entropy loss is used for both the classification and~domain loss.
The optimiser is SGD with the learning rate schedule and~momentum 0.9,
the network is initialised as our baseline model,
and the batch size is 128
where the first half is source domain data and the second half is the target domain data.

\begin{figure}
\begin{center}
\subfloat[
	For high values of \(\gamma\) DANN diverges.
]{\includegraphics[width=.75\textwidth]{img/dann_losses.pdf}}\\
\subfloat[
	\(F_1\) score regresses for high values of \(\gamma\).
]{\includegraphics[width=.75\textwidth]{img/dann_f1.pdf}}
\end{center}
\caption[Training of Domain-Adversarial Neural Network]{
	We could not converge DANN while keeping the \(F_1\) score low
	even though we did hyperparameter optimisation.
}
\label{dann_training}
\end{figure}

Although following the original implementation as closely as possible
and~doing hyperparameter optimisation of \(\gamma \in \{0.1, 0.3, 1, 3, 10\}\)
we were not able to get reasonable results with DANN.
We infer from Figure~\ref{dann_training} that
if gamma is high, the training will diverge.
On the other hand, if gamma is low \(F_1\) score regress.

\subsection{DRCN: Deep Reconstruction-Classification Network}

The last deep domain adaptation model is \textit{Deep Reconstruction-Classification Network} (DRCN)
which uses a reconstruction of target data as an auxiliary task.
Intuition is that the auxiliary task will enforce the network to capture also the structure of target data space.
More detail are provided in~Subsection~\ref{reconstruction_da}.

We followed the original implementation\footnote{Available from: \url{https://github.com/ghif/drcn}} of DRCN~\cite{ghifary2016}.
There is one big difference between the original paper and the official paper
that is the order of~training loops.
The paper states that the network in an epoch
should firstly be trained for classification task
and then for reconstruction task.
The~implementation does it the other way around.
We choose the working implementation and trained for reconstruction first.

According to the implementation, we used Adam optimiser with batch size 128 for 20 epochs and Xavier initialisation.
The trade-off parameter \(\lambda\) was set to 0.5.
Figure~\ref{reconstruction} shows that the network is able to learn a good representation that can suppress noise
while maintaining important spectral lines.

\begin{figure}
\begin{center}
\includegraphics[width=.75\textwidth]{img/drcn_f1.pdf}
\end{center}
\caption[Comparison of \(F_1\) score of DRCN and LeNet-5]{
	Although the final \(F_1\) score of DRCN is higher than the \(F_1\) score of LeNet-5,
	the full progress shows that the result is not significant.
}
\label{drcn_f1}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{img/reconstructed_spectra.pdf}
\caption[Spectra reconstructed with convolutional autoencoder]{
	The two spectra reconstructed with the convolutional autoencoder in DRCN shows that the training was successful.
	The autoencoder can reduce noise in the spectra
	while still keeping track of spectral lines.
}
\label{reconstruction}
\end{figure}

However, achieving poor results with previous methods,
we did not suppose to get a better result now.
The source \(F_1\) score is 0.9393 and the target \(F_1\) score 0.2898,
which is better than baseline
but the development of the \(F_1\) score in Figure~\ref{drcn_f1} suggest no significance.
Target precision is 17.97\% which is the~best so far,
but target recall 74.74\% is the worst.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|}
	\hline
	Predicted class & \multicolumn{2}{c|}{Actual class} \\
	\hline \hline
	& QSO & non-QSO \\ \hline
	QSO & 142 & 648 \\ \hline
	non-QSO & 48 & 49\,162 \\ \hline
\end{tabular}
\end{center}
\caption{Confusion matrix of DRCN for target data}
\end{table}

\section{Discussion of Experiments}

We summarise the results of our baseline and deep domain adaptation methods in Table~\ref{summary}.
We do not include the performance of DANN
because we were not able to train it correctly even though we did hyperparameter optimisation.
Table~\ref{summary} clearly shows that domain adaptation based on neural networks cannot significantly improve performance in comparison to baseline
when applied to astronomical data.
We would expect an increase in a metric of at least 5\% as in the original paper of the deep domain methods on standard academical datasets.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|r|}
	\hline
	Method & Source \(F_1\) & Target \(F_1\) & Precision (\%) & Recall (\%) \\
	\hline \hline
	Baseline & 0.9397 & 0.2294 & 13.48 & 76.84 \\ \hline
	DDC & 0.9354 & 0.2005 & 11.51 & 77.89 \\ \hline
	Deep CORAL & 0.9396 & 0.2509 & 14.97 & 77.37 \\ \hline
	DRCN & 0.9393 & 0.2898 & 17.97 & 74.74 \\ \hline
\end{tabular}
\end{center}
\caption{Summary table of results of experiments}
\label{summary}
\end{table}

Domain adaptation did not succeed,
although the distributions of~the~source and target domain are different.
We prove the difference in Subsection~\ref{comparison},
where we compared the two surveys.
Furthermore, we confirmed the discrepancy with dimensionality reduction techniques.
All three PCA, t-SNE and~UMAP shows that the data does not occupy a single cluster,
but the SDSS spectra concentrate in the centre and LAMOST spectra on the edges
(see Figures~\ref{pca}, \ref{tsne} and \ref{umap}).

Also, the deep domain adaptation method behaved correctly.
DDC kept the MMD between the source and target distributions low,
as shown in~Figure~\ref{ddc_mmds}.
Deep CORAL achieved the same with the CORAL metric for distribution difference
(see Figure~\ref{deep_coral_training}),
and DRCN learnt the auxiliary reconstruction task
that is supposed to support domain adaptation
(see Figure~\ref{reconstruction}).
Moreover, hyperparameter optimisation cannot improve the performance of~the~deep domain adaptation methods.

All in all, we hypothesise the problem is in the data.
Therefore, we visualise the incorrect classification of the baseline
(the errors were almost the~same for~all the methods).
In Figure~\ref{src_fp} and Figure~\ref{src_fn},
we display random spectra from source false positives and source false negatives, respectively.
At~the~same time, we display target false positives and target false negatives in Figure~\ref{trg_fp} and Figure~\ref{trg_fn}, respectively.
The rest of random spectra is in~Appendix~\ref{spectra_appendix}.

We believe the misclassifications are evidence for our conclusion
that problem is in our imperfect datasets.
The incorrectly classified examples are QSOs not yet identified by a catalogue of the surveys.
There are also spectra incorrectly classified as QSOs by the official catalogues.
Moreover, there are spectra with artefacts
(for example, missing measurements at paricular wavelengths, wrong extraction from CCD chip). 
However, the original deep domain adaptation methods are trained on well-prepared and clean data.
For~example, all data in the MNIST~\cite{lecun1998} or USPS~\cite{hull1994} are well-defined digits,
and the same applies to the Office dataset~\cite{saenko2010} commonly used as a domain adaptation benchmark.
Such well-formed datasets provide a comfortable environment for basic research.
On the other side, they do not resemble the real world or scientific situation.
That is a big issue for the application of such method to scientific data.
As we have shown, astronomy provides such a volume of~data that is impossible to make clean.
Therefore, we need either robust machine learning algorithms
or automatic procedures that clean data possibly also based on machine learning.
However, by cleaning the data, we might lose some interesting objects with strange physical properties.
Maybe, the imperfect data problem is the reason
why previous applications of domain adaptation in astronomy used active learning (a human expert)
after domain adaptation (see Section~\ref{da_astronomy}).

\begin{figure}
\subfloat[Spectrum \texttt{spec-0813-52354-0020} is a QSO.]{
\includegraphics[width=\textwidth]{img/src_fn/spec-0813-52354-0020.pdf}
}\\
\subfloat[Spectrum \texttt{spec-0967-52636-0214} is a QSO.]{
\includegraphics[width=\textwidth]{img/src_fn/spec-0967-52636-0214.pdf}
}\\
\subfloat[Spectrum \texttt{spec-1199-52703-0317} has no visual features of a QSO.]{
\includegraphics[width=\textwidth]{img/src_fn/spec-1199-52703-0317.pdf}
}\\
\subfloat[Spectrum \texttt{spec-1992-53466-0317} has no visual features of a QSO.]{
\includegraphics[width=\textwidth]{img/src_fn/spec-1992-53466-0317.pdf}
}\\
\subfloat[Spectrum \texttt{spec-2656-54484-0409} is a QSO.]{
\includegraphics[width=\textwidth]{img/src_fn/spec-2656-54484-0409.pdf}
}
\caption[The first part of sample of source false negatives]{
	The first part of sample of source false negatives resembles
	that the CNN incorrectly classifies some true QSOs.
	However, there are also spectra not clearly QSOs.
}
\label{src_fn}
\end{figure}

\begin{figure}
\subfloat[Spectrum \texttt{spec-0406-51900-0598} is a QSO probably not yet identified by SDSS.]{
\includegraphics[width=\textwidth]{img/src_fp/spec-0406-51900-0598.pdf}
}\\
\subfloat[Spectrum \texttt{spec-0560-52296-0199} is a QSO probably not yet identified by SDSS.]{
\includegraphics[width=\textwidth]{img/src_fp/spec-0560-52296-0199.pdf}
}\\
\subfloat[Spectrum \texttt{spec-0620-52081-0223} has some emission lines, but it is not clear if it is a QSO.]{
\includegraphics[width=\textwidth]{img/src_fp/spec-0620-52081-0223.pdf}
}\\
\subfloat[Spectrum \texttt{spec-0713-52178-0031} is a QSO probably not yet identified by SDSS.]{
\includegraphics[width=\textwidth]{img/src_fp/spec-0713-52178-0031.pdf}
}\\
\subfloat[Spectrum \texttt{spec-0769-54530-0502} is a QSO probably not yet identified by SDSS.]{
\includegraphics[width=\textwidth]{img/src_fp/spec-0769-54530-0502.pdf}
}
\caption[The first of part sample of source false positives]{
	The first of part sample of source false positives shows that
	they contain a significant amount of QSOs not yet in the catalogues of QSOs of the SDSS.
}
\label{src_fp}
\end{figure}

\begin{figure}
\subfloat[Spectrum \texttt{spec-56627-HD095359N274143M01\_sp09-194} is a QSO.]{
\includegraphics[width=\textwidth]{img/trg_fn/spec-56627-HD095359N274143M01_sp09-194.pdf}
}\\
\subfloat[Spectrum \texttt{spec-57163-HD163226N274234M01\_sp13-166} cannot be clearly identified as~a~QSO.]{
\includegraphics[width=\textwidth]{img/trg_fn/spec-57163-HD163226N274234M01_sp13-166.pdf}
}\\
\subfloat[Spectrum \texttt{spec-57284-EG234322N101953M01\_sp04-154} cannot be clearly identified as~a~QSO.]{
\includegraphics[width=\textwidth]{img/trg_fn/spec-57284-EG234322N101953M01_sp04-154.pdf}
}\\
\subfloat[Spectrum \texttt{spec-57367-GAC100N13M1\_sp14-011} cannot be clearly identified as a~QSO.]{
\includegraphics[width=\textwidth]{img/trg_fn/spec-57367-GAC100N13M1_sp14-011.pdf}
}\\
\subfloat[Spectrum \texttt{spec-57388-EG015238N022953M01\_sp11-103} cannot be clearly identified as~a~QSO.]{
\includegraphics[width=\textwidth]{img/trg_fn/spec-57388-EG015238N022953M01_sp11-103.pdf}
}
\caption[The first part of sample of target false negatives]{
	The first part of sample of target false negatives reveals
	they contain spectra not clearly QSOs from the visual perspective.
}
\label{trg_fn}
\end{figure}

\begin{figure}
\subfloat[Spectrum \texttt{spec-56201-EG214025S065830V02\_sp16-165} is a QSO probably not yet identified by LAMOST.]{
\includegraphics[width=\textwidth]{img/trg_fp/spec-56201-EG214025S065830V02_sp16-165.pdf}
}\\
\subfloat[Spectrum \texttt{spec-56225-GAC051N24B1\_sp10-104} contains bad pixels.]{
\includegraphics[width=\textwidth]{img/trg_fp/spec-56225-GAC051N24B1_sp10-104.pdf}
}\\
\subfloat[Spectrum \texttt{spec-56299-GAC096N32B1\_sp08-170} contains bad pixels.]{
\includegraphics[width=\textwidth]{img/trg_fp/spec-56299-GAC096N32B1_sp08-170.pdf}
}\\
\subfloat[Spectrum \texttt{spec-56304-GAC094N27M1\_sp10-105} contains bad pixels.]{
\includegraphics[width=\textwidth]{img/trg_fp/spec-56304-GAC094N27M1_sp10-105.pdf}
}\\
\subfloat[Spectrum \texttt{spec-56344-GAC088N41V3\_sp08-176} contains bad pixels.]{
\includegraphics[width=\textwidth]{img/trg_fp/spec-56344-GAC088N41V3_sp08-176.pdf}
}
\caption[The first part of sample of target false positives]{
	The first part of sample of target false positives resembles
	that they contain spectra with bad pixels
	and a QSO that is not yet identified.
}
\label{trg_fp}
\end{figure}

\chapter{Domain Adaptation (DA)}
\label{da_chapter}

Survey the current state of domain adaptation using neural network models in machine learning and with a focus on astronomical applications.

% TODO include examples from astronomical spectroscopy

\section{DA in Context of Machine and Transfer Learning}

Now, we introduce the crucial concept of our work: domain adaptation.
Domain adaptation is a subfield of transfer learning,
which is part of machine learning.
Transfer learning is defined in most papers regarding the survey by Pan and Yang~\cite{pan2010}.
A more recent survey by Weiss, Khoshgoftaar and Wang~\cite{weiss2016} has the benefit
that it contains newer methods than the survey by Pan and Yang~\cite{pan2010},
but its definition of transfer learning and domain adaptation is the same.

Machine learning has a common assumption that training and test data are independent and identically distributed
that means samples are drawn from the same feature space and the same distribution.~\cite{daume2006}
When this assumption does not hold transfer learning and domain adaptation come into play.
Moreover, there is a biological inspiration
because humans seem to have natural ways to transfer knowledge from previous experience to new challenges.~\cite{torrey2010}

% TODO define TL
Pan and Yang~\cite{pan2010} define transfer learning
as the ability of a system to recognise and apply knowledge and skill
learned in previous tasks to novel tasks
and they introduce the notion of a domain and a task.

A domain consists of two components: a feature space and a marginal probability distribution.

\begin{definition}
	A domain \(\mathcal{D} = [X, P(X)]\)
	is a~\(d\)-dimensional feature space \(X \subset \mathbb{R}^d\)
	with a~marginal provavility distribution \(P(X)\).~\cite{pan2010}
\end{definition}

Given a specific domain, a task consists of two components: a label space and an objective predictive function
which is not observed but learned from training data.

\begin{definition}
	A task \(\mathcal{T} = [Y, P(Y | X)]\)
	is a label space \(Y\)
	and a conditional probability distribution \(P(Y | X)\).~\cite{pan2010}
\end{definition}

When we are given a transfer learning problem,
we have to identify a source domain and a source learning task,
a target domain and a target learning task.
Then, transfer learning aims to help improve the learning of the target predictive function in the target domain using knowledge in the source domain and the source task,
where the domains are different or the tasks are different.~\cite{pan2010}

Lastly, Torrey and Shavlik warn that both the source and target domains and tasks need to be sufficiently related
else negative transfer may occur.
Negative transfer is the situation in which the usage of source data degrades the performance.
On the other side, when the performance is improved,
we talk about a positive transfer.~\cite{torrey2010}

TL helps to exploit data or models available in similar domains.~\cite{csurka2017}

\section{Theory and Formalization of DA}

% TODO define DA
Domain adaptation (DA) is a particular case of TL
that leverages labelled data from source domain to learn a classifier for a target domain while the tasks are the same.
It is assumed that source and target domains are not identical but related.
If they were identical it would be a standard machine learning problem.
Therefore there is a distribution discrepancy between the source and target domains (\textit{domain shift}).~\cite{csurka2017}

\begin{definition}
	\textit{Domain adapatiton} is the scenario when the source
	\(\mathcal{D}^s = [X^s, P(X^s)]\)
	and target \(\mathcal{D}^t = [X^t, P(X^t)]\)
	are different (\(\mathcal{D}^s \ne \mathcal{D}^t\))
	but the source \(\mathcal{T}^s\) and target \(\mathcal{T}^t\)
	tasks are the same (\(\mathcal{T}^s = \mathcal{T}^t\)).
	% TODO should be the second condition always true for DA
	The first condition implies \(X^s \ne X^t\),
	\(P(X^s) \ne P(X^t)\) or both are true.~\cite{pan2010}
\end{definition}

% TODO refer to specific section
In our case, the two archive have different target selection strategies
and the instrument are different.

\begin{definition}
	\textit{Homogeneous} DA is setting in which the source \(X^s\)
	and target \(X^t\) feature spaces are the same
	(\(X^s = X^t\))
	while in \textit{heterogeneous} the source and target spaces are different
	(\(X^s \ne X^t\)).~\cite{csurka2017}
\end{definition}

% TODO check feature space
Our case is the \textit{homogeneous} DA where the feature spaces are identical
(\(X^s = X^t\))
still the source and target data are have different distributions
(\(P(X^s) \ne P(X^t)\)).
Therefore, further we focus on homogeneous DA.
because our spectra prepared into a same feature space (\(\mathbb{R}^{3659}\)).

Next we distinguish \textit{supervised}, \textit{semi-supervised}
and \textit{unsupervised} DA
according to availability of labels in the target domain:

\begin{itemize}
	\item superivesed DA: labelled data in the target domain are present;
	\item semi-supervised DA: we have only very limited amount of labelled data in the target domain while most of the data is unlabelled;
	\item unsupervised DA: no labelled data are available in the target domain.
\end{itemize}

We are concern with \textit{unsupervised DA} because we have
% TODO refer to a section
no reliable labelled data in the target domain.~\cite{wang2018}
In corespondence with~\cite{ganin2016},
we consider a classification problem within an input space \(X\)
with \(L\) possible labels from a set \(Y = {0, \dots, L - 1}\).
The source \(\mathcal{D}_s\) and target \(\mathcal{T}_s\) domains
have different distibutions over \(X \times Y\).
From the source and target domains
we have \textit{labelled source sample} \(S\) drawn i.i.d. from \(\mathcal{D}_s\)
and \textit{unlabelled target sample} \(T\) drawn i.i.d. from \(\mathcal{D}^X_t\) 
(the marginal distribution of \(\mathcal{D}_t\) over \(X\)):

\begin{align}
	S &= \{(\mathbf{x}_i, y_i)\}^n_{i = 1} \sim (\mathcal{D}_s)^n, \\
	T &= \{\mathbf{x}_i\}^N_{i = n + 1} \sim (\mathcal{D}^X_t)^{n'},
\end{align}

where \(N = n + n'\) is the total number of examples (data points).

Nowadays, methods of DA are generally distinguished into to categories:
\textit{shallow DA} methods that can be applied on extracted vectors of features
and \textit{deep DA} methods based on neural networks and deep learning
augemented for domain adapation.~\cite{csurka2017}

According to Wang and Deng~\cite{wang2018} shallow DA methods
can be categorised into two classes:
\textit{instance-based DA}
(the discrepancy is reduces by reweighting the source instances)
and \textit{feature-based DA}
(tries to learn a common shared space in which the discrepancy diminishes).
However, shallow DA methods are not interesting for us
because they do not utilize neural network.
Still one possibility is to use a neural network as a feature extractor.~\cite{csurka2017}

The following section introduces the other category of deep DA
which takes the advantage of neural networks.

\section{Previous Applications of DA in Astronomy}

% TODO something like: so far shallow but some deep DA not sufficient

As we have shown, domain adaptation is of great interest to astronomers
because of different instruments, measurements and observation distribution.
Therefore, we survey the current state of domain adaptation in astronomical applications in this section.

If we have a common set of observed stars in both archives,
then we can map them and learn a transfer function.
Ho et al.~\cite{ho2017} did exactly that
% TODO add APOGEE to acronyms
because they found a common set of 9\,952 spectra in both APOGEE and LAMOST archive.
Using that set, they trained the Cannon method~\cite{ness2015},
and they used the model to transfer some physical parameters from APOGEE to LAMOST.

In the case, when there is no common set Gupta et al. experimented with subspace alignment~\cite{fernando2014} and kernel mean matching~\cite{gretton2009} followed by active learning.~\cite{gupta2016}
In the case of subspace alignment, the negative transfer occurred while the kernel mean matching seems very promising in the task of supernova classification.
Then, Vilalta et al.~\cite{vilalta2018} extended the work of Gupta et al.~\cite{gupta2016}.
Vilalta et al. used a maximum a posteriori (MAP) approach to learn a prior on the model parameters from a spectroscopic source domain
and then use this prior distribution to learn a model in a photometric target domain.
Concretely, Vilalta et al. put a prior on the number of layers of a neural network
and then used active learning.
Richards et al.~\cite{richards2011} faced a similar situation, as Gupta et al.
Richards et al. introduce the problem as sample selection bias~\cite{shimodaira2000} or covariate shift~\cite{heckman1979}
when different distributions generate the source and target data.
That is precisely the problem we have defined as domain adaptation.
Richards et al. experimented with random forest in combination with three domain adaptation methods:
importance weighting~\cite{shimodaira2000}, co-training~\cite{blum1998} and active learning~\cite{settles2009}.
The result is not surprising from our point of view.
Active learning works best while importance weighting and co-training achieve negative transfer.

The term transfer learning has been recently used in the context of deep learning.
However, transfer learning in the context of deep learning means something more concrete than what we defined as transfer learning previously.
Transfer learning in the context of deep learning is the specific situation
when a pretrained deep neural network model is taken,
and its last layers are retrained with the target domain data.
Ackermann et al.~\cite{ackermann2018} employed with the transfer learning approach in the context of deep learning to detect galaxy merges.
Ackermann et al. took the Xception convolutional neural network~\cite{chollet2017}
and retrain its last layers with images of galaxy merger labelled in a citizen science Galaxy Zoo project.
This transfer learning approach allowed Ackermann et al. to lower the best error rate so far by 15\%.

\section{Neural Networks in DA}

Donahue et al. showed in the paper on DeCAF~\cite{donahue2014}
that deep neural networks learn more transferable features
but the performance is still affected by the domain shift.

% TODO write about DeCAF

The group of DA methods using neural networks is called \textit{deep DA}
and Wang \& Deng define it in~\cite{wang2018}:

% TODO broad and narrow sense
\begin{definition}
	Deep DA is a method that utilizes a deep network to enhance the performance of DA.
\end{definition}

Approaches to deep DA were originally divided by Csurka in~\cite{csurka2017}
into three categories according to training loss.
Then, Wang and Deng further improve and detail the categorisation in~\cite{wang2018}:

\begin{itemize}
	\item \textit{discrepancy-based} DA fine-tunes the neural network
		with target data to diminish the domain shift;
	\item \textit{adversarial-based} DA encorage domain confusion
		by using discriminators with adversarial objective; and
	\item \textit{reconstruction-based} DA uses data reconstruction auxialiary task to learn domain invariant features.
\end{itemize}

We summarize the deep DA methods categorisation in the mind map of Figure~\ref{mind_map}.

\begin{figure}
	\begin{tikzpicture}
		\path[mindmap, concept color=black, text=white]
		node[concept] {Deep DA}
		[clockwise from=60]
		child[concept] {
			node[concept] {Adversarial}
			[clockwise from=75]
			child {node[concept] {generative model}}
			child {
				node[concept] {non-generative model}
				child {node[concept] {DANN}}
				child {node[concept] {ADDA}}
				}
			}
		child[concept] {
			node[concept] {Recon\-struction}
			[clockwise from=30]
			child {node[concept] {adversarial model}}
			child {
				node[concept] {encoder-decoder model}
				child {node[concept] {SDA}}
				child {node[concept] {mSDA}}
				child {node[concept] {DRCN}}
				child {node[concept] {DSN}}
				}
			}
		child[concept] {
			node[concept] {Discrepancy}
			[clockwise from=30]
			child {node[concept] {class criterion}}
			child {
				node[concept] {statistic criterion}
				child {node[concept] {DDC}}
				child {node[concept] {DAN}}
				child {node[concept] {JAN}}
				child {node[concept] {Deep CORAL}}
				}
			child {node[concept] {archi\-tecture criterion}}
			child {node[concept] {geometric criterion}}
			};
	\end{tikzpicture}
	\caption{Mind map of deep DA}
	\label{mind_map}
\end{figure}

\subsection{Discrepancy-based DA}

Discrepancy-based deep DA uses either labelled or unlabelled target data
to fine-tune a neural network to diminish the dicrepancy
between the source and target domains.

The discrepancy-based method divided based on
\textit{class}, \textit{statistic}, \textit{architecture} and \textit{geometric}
criterion by~\cite{wang2018}.

DDC~\cite{tzeng2014}, DAN~\cite{long2015}, JAN~\cite{long2017}
and Deep CORAL~\cite{sun2016}.

The essence of Deep Domain Confusion (DCC)~\cite{tzeng2014} is finding domain invariant representation
while learning to predict class labels.
The intuition behind DDC is that learning represenation
that minimises the distance between source and target distributions
then a classifier trains on source labelled data
can be directly applied to the target domain achieving minimal loss in accuracy.
DDC optimises loss function that includes both prediciton error and \textit{domain confusion} loss to learn deep representation.
The invariant deep repsentation is achived by incorporating an adaptation layer
into a deep CNN with a domain confusion loss computed via \textit{maximum mean discrepancy} (MMD).
MMS is a standard distirbution distance metric
witch is empirically approximated as follows:

\begin{equation}
	\mathrm{MMD}(S, T) = \left\|
	\frac{1}{n} \sum_{i = 1}^{n} \phi(\mathbf{x}_i) -
	\frac{1}{n'} \sum_{i = n + 1}^{N} \phi(\mathbf{x}_i)
	\right\|,
	\label{maximam_mean_discrepancy}
\end{equation}

where \(\phi(\cdot)\) is a represenation
that operates on both source \(\mathbf{s}_i \in S\) and target \(\mathbf{t}_i \in T\) data points.
Using the adaptation layer with the domain loss function DDC claims
to learn representation that is both domain invariant
but still offers strong semantic separation.
However, DDC also want to learn representation
that enables to learn a strong classifier.
Therefore is minimizes a loss that incorporates both domain confusion loss and classification loss:

\begin{equation}
	\mathcal{L}_{\mathrm{DDC}} = \mathcal{L}_c(S)
	+ \lambda \mathrm{MMD}^2(S, T),
\end{equation}

where \(\mathcal{L}_c\) stands for classification loss on the labelled source data \(S\).
To control the stregnth of domain confusion DDC introduces the hyperparameter~\(\lambda\)
(DCC uses \(\lambda = 0.25\) in its experiments).

% TODO cite AlexNet and explain what is pre-trained (cite ImageNet)
In experiments DDC, uses \textit{pre-trained} AlexNet with a lower dimensional \textit{bottleneck} adaptation layer
which should regularise the training
and prevent overfitting the source distribution.
The domain confusion loss is placed on the bottleneck adaptation layer.

% TODO cite Office benchmark and maybe describe?
DDC was evaluated of the Office benchmark achieving 81.2\% average accuracy on the target domains.

\subsection{Adversarial-based DA}

Adversarial-based deep DA utilizes a domain discriminator
that tries to distinguish whether a sample comes from source and target domains.
If we are able to confuse the discriminator
we will also achieve domain confusion through the adversarial objective.

The methods are divided into \textit{generative} and \textit{non-generative}
models by~\cite{wang2018}

DANN~\cite{ganin2016} and ADDA~\cite{tzeng2017}.

% TODO cite http://sites.skoltech.ru/compvision/projects/grl/files/paper.pdf
Domain-Adversarial Training of Neural Neural Networks (DANN)~\cite{ganin2016}
is a representation learning approach for domain adaptation.
DANN is based on idea that features for domain adaptation
cannot discriminate between source and target domains
while maintaining discriminative properties for a classification task.
DANN proposes a \textit{gradient reversal layer} (GRL)
that can be incorporated in almost any feed-forward neural network.
Therefore, DANN jointly optimises feature extractor
and two discrimivative classifier.
The first is a label predictor that predicts classes
and a domain classifier tath discriminates between source and target domains.
The feature extractor is trained to minimise the loss of the label classifier
and maximise the loss of the domain classifier.
Thus, domain classifier and feature extractor are learning in adversarial fashion and encourage domain invariant features.
DANN combines feature extractor, label predictor and domain classifier is a single architecture
and can be trained with standard backpropagation learning algorithm.

Let \(G_f(\cdot; \theta_f)\) be the \(D\)-dimensional neural network feature extractor, with parameters \(\theta_f\),
\(G_y(\cdot; \theta_y)\) be the label predictor with parameters \(\theta_y\)
that takes features from \(G_f\) as inputs and output class probabilities
and \(G_d(\cdot; \theta_d)\) be the domain classifier with parameters \(\theta_d\).

Training DANN is optimising prediction loss and domain loss:

\begin{align}
	E(\theta_f, \theta_y, \theta_d) &= \frac{1}{n} \sum^{n}_{i = 1} \mathcal{L}_y(G_y(G_f(\mathbf{x}_i; \theta_f), \theta_y), y_i) \nonumber \\
	&\qquad {} - \lambda \left(\frac{1}{n} \sum^{n}_{i = 1} \mathcal{L}_d(G_d(G_f(\mathbf{x}_i; \theta_f), \theta_d), d_i) \right. \nonumber \\
	&\qquad \left. {} + \frac{1}{n'} \sum^{N}_{i = n + 1} \mathcal{L}_d(G_d(G_f(\mathbf{x}_i; \theta_f), \theta_d), d_i)\right)
\end{align}

by finding the saddle point \(\hat{\theta}_f\), \(\hat{\theta}_y\), \(\hat{\theta}_d\) that satisfy:

\begin{align}
	(\hat{\theta}_f, \hat{\theta}_y)
	&= \operatorname*{arg\,min}_{\theta_f, \theta_y} E(\theta_f, \theta_y, \hat{\theta}_d),\\
	\hat{\theta}_d
	&= \operatorname*{arg\,max}_{\theta_d} E(\hat{\theta}_f, \hat{\theta}_y, \theta_d).
\end{align}

where \(\mathcal{L}_y\) is a classification loss,
\(\mathcal{L}_d\) is a domain loss,
\(\lambda\) is a trade-off hyperparameter
and \(d_i\) is a binary variable:

\begin{equation}
	d_i =
	\begin{cases}
		0 & \quad \text{if } i \in \{1, \dots, n\} \\
		1 & \quad \text{if } i \in \{n + 1, \dots, N\}
	\end{cases}
\end{equation}

The saddle points can be found using gradient updates:

\begin{align}
	\theta_f &\gets \theta_f - \mu \left(
	\frac{\partial \mathcal{L}_y^i}{\partial \theta_f}
	- \lambda \frac{\partial \mathcal{L}_d^i}{\partial \theta_f}
	\right),
	\label{feature_parameters_update} \\
	\theta_y &\gets \theta_y - \mu
	\frac{\partial \mathcal{L}_y^i}{\partial \theta_y}, \\
	\theta_d &\gets \theta_d - \mu \lambda
	\frac{\partial \mathcal{L}_d^i}{\partial \theta_d}.
\end{align}

The previous updates are very similar to SGD.
However, the update~\ref{feature_parameters_update} has substraction in it instead of addition.

DANN overcomes the substruction by incorporating a gradient reversal layer \(\mathcal{R}(\mathbf{x})\) between the feature extrator and domain classifier.
Now, DANN seamlesly works with SGD.
The gradient reversal layer has no parameters
and during forward propagation the GRL acts as identity:

\begin{equation}
	\mathcal{R}(\mathbf{x}) = \mathbf{x}.
\end{equation}

However, in backpropagation it negates the gradient:

\begin{equation}
	\frac{d \mathcal{R}}{d \mathbf{x}} = -\mathbf{I},
\end{equation}

where \(\mathbf{I}\) is identity matrix.

The objective to be optimised with gradient descent:

\begin{align}
	E(\theta_f, \theta_y, \theta_d) &= \frac{1}{n} \sum^{n}_{i = 1} \mathcal{L}_y(G_y(G_f(\mathbf{x}_i; \theta_f), \theta_y), y_i) \nonumber \\
	&\qquad {} - \lambda \left(\frac{1}{n} \sum^{n}_{i = 1} \mathcal{L}_d(G_d(\mathcal{R}(G_f(\mathbf{x}_i; \theta_f)), \theta_d), d_i) \right. \nonumber \\
	&\qquad \left. {} + \frac{1}{n'} \sum^{N}_{i = n + 1} \mathcal{L}_d(G_d(\mathcal{R}(G_f(\mathbf{x}_i; \theta_f)), \theta_d), d_i)\right)
\end{align}

The gradint reversal layer changes the update~\ref{feature_parameters_update}
to version fully compatible with SGD:

\begin{equation}
	\theta_f \gets \theta_f - \mu \left(
	\frac{\partial \mathcal{L}_y^i}{\partial \theta_f}
	+ \lambda \frac{\partial \mathcal{L}_d^i}{\partial \theta_f}
	\right).
\end{equation}

\subsection{Reconstruction-based DA}

The last group of methods is based on data reconstruction auxialiary task.
The reconstructor forces to find a shared representation
of a source and target domains.

According to~\cite{wang2018} subgroups are \textit{encoder-decoder} and
\textit{adversarial} reconstruction.

SDA~\cite{vincent2008, glorot2011}, mSDA~\cite{chen2012}, DRCN~\cite{ghifary2016} and DSN~\cite{bousmalis2016}.

% TODO check and verify
SDA and mSDA were developed for Amazon reviews benchmark textual data
while DRCN and DSN for visual data.
Therefore we focus on DRCN as it is simpler to train than DSN.

Deep Reconstruction-Classification Network (DRCN)~\cite{ghifary2016} is a CNN
that learn both supervised souce labelled classification and unsupervised target data reconstruction.
Encoder is shared between both task but decoding parameters are separete.
The data reconstruction task is a auxiliary task supposed to learn good feature representation beneficial for both source and target domain.

The intuition behind DRCN is that good represenstation for DA
should capture both the properties for classification
and data structure (reconstruct well the target domain).
DRCN is composed of two pipelines.
The first one is source label predictor
and the second is a convolutional autoencoder fro target data reconstruction.
The model is jointly optimised for classification and reconstruction tasks.
DRCN hopes that the auxiliary reconstruction task will learn good repreresentation for target classification.

The function \(F_c: X \to Y\) is the label predictor
and \(F_r: X \to X\) is the data reconstructor.
The two functions are composed out of three components:

\begin{itemize}
	\item an encoder feature mapping \(G_e: X \to Z\);
	\item a decoder \(G_d: Z \to X\); and
	\item a classificator \(G_l: Z \to Y\);
\end{itemize}

where \(Z\) is a latent feature space where DRCN encodes data.
Given the above component,
\(F_c(\cdot; \theta_c) = G_l(G_e(\cdot; \theta_e); \theta_l)\)
and \(F_r(\cdot; \theta_r) = G_d(G_e(\cdot; \theta_e); \theta_d)\)
where \(\theta_c = \{\theta_e, \theta_l\}\)
and \(\theta_r = \{\theta_d, \theta_e\}\) are parameters of DRCN.
Note that \(\theta_e\) is shared between the label predictor and the autoencoder.

Therefore, the learning objective of DRCN contains classification and reconstruction loss respectively:

\begin{align}
	\mathcal{L}_c(S) &= \sum_{i = 1}^{n}
	l_c(F_c(\mathbf{x}_i; \theta_c), y_i),
	\label{drcn_classification_loss} \\
	\mathcal{L}_r(T) &= \sum_{i = n + 1}^{N}
	l_r(F_r(\mathbf{x}_i; \theta_r), \mathbf{x}_i),
	\label{drcn_reconstruction_loss}
\end{align}

wher \(l_c\) is a form of cross entropy loss
and \(l_r\) is the squared loss:

\begin{equation}
	l_r = \|\mathbf{x}_i - F_r(\mathbf{x}_i)\|_2^2.
\end{equation}

By combining the terms~\ref{drcn_classification_loss} and~\ref{drcn_reconstruction_loss},
DRCN aims to minimise following objective:

\begin{equation}
	\mathcal{L}_{\mathrm{DDC}}
	= \lambda \mathcal{L}_c(S) + (1 - \lambda) \mathcal{L}_r(T),
	\label{drcn_loss}
\end{equation}

where \(\lambda \in [0; 1]\) is a hyperparameter of the trade-off between the two loss functions.
The objective~\ref{drcn_loss} can be optimised with SGD.
